model:
  ollama_model: "llama3.2:1b"  
  ollama_url: "http://localhost:11434/api/generate"  # Ollama local API endpoint
  langchain_model: "llama"  # LangChain model to use